Matplotlib created a temporary cache directory at /tmp/matplotlib-93jv5ipx because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/opt/conda/lib/python3.11/site-packages/lightning/fabric/loggers/csv_logs.py:195: UserWarning: Experiment logs directory logs/OL2/deep exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
  rank_zero_warn(
/opt/conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory logs/OL2/deep/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type               | Params
--------------------------------------------------
0 | module     | DeepNetwork        | 131 K 
1 | model_acc  | MulticlassAccuracy | 0     
2 | model_loss | CrossEntropyLoss   | 0     
--------------------------------------------------
131 K     Trainable params
0         Non-trainable params
131 K     Total params
0.528     Total estimated model params size (MB)
/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Use GPU: True
Files already downloaded and verified
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
PLModel                                  [1, 10]                   --
├─DeepNetwork: 1-1                       --                        --
│    └─Flatten: 2-1                      [1, 3072]                 --
│    └─Sequential: 2-2                   [1, 35]                   --
│    │    └─Linear: 3-1                  [1, 35]                   107,555
│    │    └─ReLU: 3-2                    [1, 35]                   --
│    │    └─BatchNorm1d: 3-3             [1, 35]                   70
│    │    └─Dropout: 3-4                 [1, 35]                   --
│    │    └─Linear: 3-5                  [1, 35]                   1,260
│    │    └─ReLU: 3-6                    [1, 35]                   --
│    │    └─BatchNorm1d: 3-7             [1, 35]                   70
│    │    └─Linear: 3-8                  [1, 35]                   1,260
│    │    └─ReLU: 3-9                    [1, 35]                   --
│    │    └─BatchNorm1d: 3-10            [1, 35]                   70
│    │    └─Linear: 3-11                 [1, 35]                   1,260
│    │    └─ReLU: 3-12                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-13            [1, 35]                   70
│    │    └─Linear: 3-14                 [1, 35]                   1,260
│    │    └─ReLU: 3-15                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-16            [1, 35]                   70
│    │    └─Linear: 3-17                 [1, 35]                   1,260
│    │    └─ReLU: 3-18                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-19            [1, 35]                   70
│    │    └─Linear: 3-20                 [1, 35]                   1,260
│    │    └─ReLU: 3-21                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-22            [1, 35]                   70
│    │    └─Linear: 3-23                 [1, 35]                   1,260
│    │    └─ReLU: 3-24                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-25            [1, 35]                   70
│    │    └─Linear: 3-26                 [1, 35]                   1,260
│    │    └─ReLU: 3-27                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-28            [1, 35]                   70
│    │    └─Linear: 3-29                 [1, 35]                   1,260
│    │    └─ReLU: 3-30                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-31            [1, 35]                   70
│    │    └─Linear: 3-32                 [1, 35]                   1,260
│    │    └─ReLU: 3-33                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-34            [1, 35]                   70
│    │    └─Linear: 3-35                 [1, 35]                   1,260
│    │    └─ReLU: 3-36                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-37            [1, 35]                   70
│    │    └─Linear: 3-38                 [1, 35]                   1,260
│    │    └─ReLU: 3-39                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-40            [1, 35]                   70
│    │    └─Linear: 3-41                 [1, 35]                   1,260
│    │    └─ReLU: 3-42                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-43            [1, 35]                   70
│    │    └─Linear: 3-44                 [1, 35]                   1,260
│    │    └─ReLU: 3-45                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-46            [1, 35]                   70
│    │    └─Linear: 3-47                 [1, 35]                   1,260
│    │    └─ReLU: 3-48                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-49            [1, 35]                   70
│    │    └─Linear: 3-50                 [1, 35]                   1,260
│    │    └─ReLU: 3-51                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-52            [1, 35]                   70
│    │    └─Linear: 3-53                 [1, 35]                   1,260
│    │    └─ReLU: 3-54                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-55            [1, 35]                   70
│    │    └─Linear: 3-56                 [1, 35]                   1,260
│    │    └─ReLU: 3-57                   [1, 35]                   --
│    │    └─BatchNorm1d: 3-58            [1, 35]                   70
│    └─Linear: 2-3                       [1, 10]                   360
==========================================================================================
Total params: 131,925
Trainable params: 131,925
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.13
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 0.53
Estimated Total Size (MB): 0.55
==========================================================================================
/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Traceback (most recent call last):
  File "/home/rhoehn/mtsu.csci.7850/OL2/cifar10-deep.py", line 170, in <module>
    trainer.fit(model, xy_train, xy_val)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 532, in fit
    call._call_and_handle_interrupt(
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 571, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 980, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
    self.fit_loop.run()
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py", line 370, in on_advance_end
    call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 195, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 303, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 362, in _save_topk_checkpoint
    self._save_none_monitor_checkpoint(trainer, monitor_candidates)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 675, in _save_none_monitor_checkpoint
    self._remove_checkpoint(trainer, previous)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 737, in _remove_checkpoint
    trainer.strategy.remove_checkpoint(filepath)
  File "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py", line 476, in remove_checkpoint
    self.checkpoint_io.remove_checkpoint(filepath)
  File "/opt/conda/lib/python3.11/site-packages/lightning/fabric/plugins/io/torch_io.py", line 102, in remove_checkpoint
    fs.rm(path, recursive=True)
  File "/opt/conda/lib/python3.11/site-packages/fsspec/implementations/local.py", line 170, in rm
    os.remove(p)
FileNotFoundError: [Errno 2] No such file or directory: '/home/rhoehn/mtsu.csci.7850/OL2/logs/OL2/deep/checkpoints/epoch=0-step=160-v18.ckpt'
