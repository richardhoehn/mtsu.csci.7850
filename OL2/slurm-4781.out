Matplotlib created a temporary cache directory at /tmp/matplotlib-sw6q33le because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/opt/conda/lib/python3.11/site-packages/lightning/fabric/loggers/csv_logs.py:195: UserWarning: Experiment logs directory logs/OL2/wide exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
  rank_zero_warn(
/opt/conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory logs/OL2/wide/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type               | Params
--------------------------------------------------
0 | module     | WideNetwork        | 129 K 
1 | model_acc  | MulticlassAccuracy | 0     
2 | model_loss | CrossEntropyLoss   | 0     
--------------------------------------------------
129 K     Trainable params
0         Non-trainable params
129 K     Total params
0.519     Total estimated model params size (MB)
/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Use GPU: True
Files already downloaded and verified
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
PLModel                                  [1, 10]                   --
├─WideNetwork: 1-1                       --                        84
│    └─Flatten: 2-1                      [1, 3072]                 --
│    └─Linear: 2-2                       [1, 42]                   129,066
│    └─ReLU: 2-3                         [1, 42]                   --
│    └─BatchNorm1d: 2-4                  [1, 42]                   84
│    └─Dropout: 2-5                      [1, 42]                   --
│    └─Linear: 2-6                       [1, 10]                   430
==========================================================================================
Total params: 129,664
Trainable params: 129,664
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.13
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.00
Params size (MB): 0.52
Estimated Total Size (MB): 0.53
==========================================================================================
/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=50` reached.

Processing Time: 112.802832 seconds

Validation accuracy: 0.38769999 0.40450001 0.42550001 0.43390000 0.44130000 0.45109999 0.44610000 0.44740000 0.45980000 0.45850000 0.45510000 0.45980000 0.46340001 0.46560001 0.47080001 0.46759999 0.46869999 0.46509999 0.47110000 0.47110000 0.47369999 0.47020000 0.47270000 0.47860000 0.47540000 0.47880000 0.47790000 0.47520000 0.47940001 0.47729999 0.47909999 0.48179999 0.48140001 0.47909999 0.48620000 0.48170000 0.48170000 0.47970000 0.48190001 0.47950000 0.48230001 0.47319999 0.47700000 0.48210001 0.48310000 0.47799999 0.47700000 0.47990000 0.47639999 0.47810000


